{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLY HMM from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['nitish','loves','facebook','can','google','will','ankita']\n",
    "sentences = [\n",
    "    'Nitish Loves Facebook',\n",
    "    'Can Nitish google facebook',\n",
    "    'Will ankita google facebook',\n",
    "    'Ankita loves Will',\n",
    "    'Will loves google'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: [('S', 'START'), ('nitish', 'NOUN'), ('loves', 'VERB'), ('facebook', 'NOUN'), ('E', 'END')]\n",
      "Sentence 2: [('S', 'START'), ('can', 'AUX'), ('nitish', 'NOUN'), ('google', 'VERB'), ('facebook', 'NOUN'), ('E', 'END')]\n",
      "Sentence 3: [('S', 'START'), ('will', 'AUX'), ('ankita', 'NOUN'), ('google', 'VERB'), ('facebook', 'NOUN'), ('E', 'END')]\n",
      "Sentence 4: [('S', 'START'), ('ankita', 'NOUN'), ('loves', 'VERB'), ('will', 'NOUN'), ('E', 'END')]\n",
      "Sentence 5: [('S', 'START'), ('will', 'NOUN'), ('loves', 'VERB'), ('google', 'NOUN'), ('E', 'END')]\n"
     ]
    }
   ],
   "source": [
    "# doc1 = sentences[0]\n",
    "# doc2 = sentences[1]\n",
    "# doc3 = sentences[2]\n",
    "# doc4 = sentences[3]\n",
    "# doc5 = sentences[4]\n",
    "\n",
    "# docs = [doc1,doc2,doc3,doc4,doc5]\n",
    "# for doc in docs:\n",
    "#     x = nlp(doc)\n",
    "#     for word in x:\n",
    "#         print(word.text,'-->', word.pos_ ,word.tag_ , spacy.explain(word.tag_))\n",
    "\n",
    "\n",
    "tagged_sentences = []\n",
    "for s in sentences:\n",
    "    doc = nlp(s)\n",
    "    tag = [(word.text.lower(), word.pos_) for word in doc]\n",
    "    tagged_sentences.append(tag)\n",
    "\n",
    "# Modify \"Will\" in the last two sentences to be a noun and \"google\" as verb\n",
    "tagged_sentences[1] = [(word, pos) if word != 'google' else (word, 'VERB') for word, pos in tagged_sentences[1]]\n",
    "tagged_sentences[2] = [(word, pos) if word != 'google' else (word, 'VERB') for word, pos in tagged_sentences[2]]\n",
    "tagged_sentences[3] = [(word, pos) if word != 'will' else (word, 'NOUN') for word, pos in tagged_sentences[3]]\n",
    "tagged_sentences[4] = [(word, pos) if word != 'will' else (word, 'NOUN') for word, pos in tagged_sentences[4]]\n",
    "\n",
    "for i,s in enumerate(tagged_sentences): \n",
    "    tagged_sentences[i] = [(word,'NOUN') if pos == 'PROPN' else (word,pos) for word,pos in s]\n",
    "\n",
    "markers = []\n",
    "for tag in tagged_sentences:\n",
    "    m = [('S', 'START')] + tag + [('E', 'END')]\n",
    "    markers.append(m)\n",
    "    \n",
    "all_tags = {}\n",
    "for i, tagged in enumerate(markers):\n",
    "    all_tags[i] = tagged\n",
    "    print(f\"Sentence {i + 1}: {tagged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [('S', 'START'),\n",
       "  ('nitish', 'NOUN'),\n",
       "  ('loves', 'VERB'),\n",
       "  ('facebook', 'NOUN'),\n",
       "  ('E', 'END')],\n",
       " 1: [('S', 'START'),\n",
       "  ('can', 'AUX'),\n",
       "  ('nitish', 'NOUN'),\n",
       "  ('google', 'VERB'),\n",
       "  ('facebook', 'NOUN'),\n",
       "  ('E', 'END')],\n",
       " 2: [('S', 'START'),\n",
       "  ('will', 'AUX'),\n",
       "  ('ankita', 'NOUN'),\n",
       "  ('google', 'VERB'),\n",
       "  ('facebook', 'NOUN'),\n",
       "  ('E', 'END')],\n",
       " 3: [('S', 'START'),\n",
       "  ('ankita', 'NOUN'),\n",
       "  ('loves', 'VERB'),\n",
       "  ('will', 'NOUN'),\n",
       "  ('E', 'END')],\n",
       " 4: [('S', 'START'),\n",
       "  ('will', 'NOUN'),\n",
       "  ('loves', 'VERB'),\n",
       "  ('google', 'NOUN'),\n",
       "  ('E', 'END')]}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique POS tags: ['AUX', 'END', 'NOUN', 'START', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "tag = set()\n",
    "for tags in all_tags.values():\n",
    "    for word, pos in tags:\n",
    "        tag.add(pos)\n",
    "        \n",
    "POS_tags = sorted(tag)\n",
    "\n",
    "print(\"Unique POS tags:\", POS_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emission Counts:\n",
      "NOUN: {'nitish': 2, 'facebook': 3, 'ankita': 2, 'will': 2, 'google': 1}\n",
      "VERB: {'loves': 3, 'google': 2}\n",
      "AUX: {'can': 1, 'will': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Count occurrences\n",
    "for x in tagged_sentences:\n",
    "    for word, pos in x:\n",
    "        emission_counts[pos][word] += 1\n",
    "        \n",
    "print(\"\\nEmission Counts:\")\n",
    "for pos, words in emission_counts.items():\n",
    "    print(f\"{pos}: {dict(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability\n",
      "P(nitish|NOUN) = 0.20\n",
      "P(facebook|NOUN) = 0.30\n",
      "P(ankita|NOUN) = 0.20\n",
      "P(will|NOUN) = 0.20\n",
      "P(google|NOUN) = 0.10\n",
      "P(loves|VERB) = 0.60\n",
      "P(google|VERB) = 0.40\n",
      "P(can|AUX) = 0.50\n",
      "P(will|AUX) = 0.50\n"
     ]
    }
   ],
   "source": [
    "# emmission probability\n",
    "probability = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for pos,words in emission_counts.items():\n",
    "    total = sum(words.values())\n",
    "    for word,count in words.items():\n",
    "        probability[pos][word] = count / total\n",
    "\n",
    "em_probabilities = {pos: dict(words) for pos, words in probability.items()}\n",
    "\n",
    "print('Probability')\n",
    "for pos,words in probability.items():\n",
    "    for word,prob in words.items():\n",
    "        store = f\"P({word}|{pos}) = {prob:.2f}\"\n",
    "        print(store)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOUN': {'nitish': 0.2,\n",
       "  'facebook': 0.3,\n",
       "  'ankita': 0.2,\n",
       "  'will': 0.2,\n",
       "  'google': 0.1},\n",
       " 'VERB': {'loves': 0.6, 'google': 0.4},\n",
       " 'AUX': {'can': 0.5, 'will': 0.5}}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transition Counts:\n",
      "END: {'START': 5}\n",
      "START: {'NOUN': 3, 'AUX': 2}\n",
      "NOUN: {'VERB': 5, 'END': 5}\n",
      "VERB: {'NOUN': 5}\n",
      "AUX: {'NOUN': 2}\n"
     ]
    }
   ],
   "source": [
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for sentence in markers:\n",
    "    for i in range(len(sentence)):\n",
    "        current_pos = sentence[i-1][1]\n",
    "        next_pos = sentence[i][1]\n",
    "        transition_counts[current_pos][next_pos] += 1\n",
    "\n",
    "\n",
    "print(\"\\nTransition Counts:\")\n",
    "for current_pos, transitions in transition_counts.items():\n",
    "    print(f\"{current_pos}: {dict(transitions)}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transition Probability\n",
      "P(START|END) = 1.00\n",
      "P(NOUN|START) = 0.60\n",
      "P(AUX|START) = 0.40\n",
      "P(VERB|NOUN) = 0.50\n",
      "P(END|NOUN) = 0.50\n",
      "P(NOUN|VERB) = 1.00\n",
      "P(NOUN|AUX) = 1.00\n"
     ]
    }
   ],
   "source": [
    "transition_probabity = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for current_pos,transitions in transition_counts.items():\n",
    "    total_transitions = sum(transitions.values())\n",
    "    for next_pos,count in transitions.items():\n",
    "        transition_probabity[current_pos][next_pos] = count / total_transitions\n",
    "        \n",
    "trans_probabilities = {pos: dict(transitions) for pos, transitions in transition_probabity.items()}\n",
    "\n",
    "\n",
    "print('\\nTransition Probability')\n",
    "for current_pos,transitions in transition_probabity.items():\n",
    "    for next_pos,prob in transitions.items():\n",
    "        store = f\"P({next_pos}|{current_pos}) = {prob:.2f}\"\n",
    "        print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'END': {'START': 1.0},\n",
       " 'START': {'NOUN': 0.6, 'AUX': 0.4},\n",
       " 'NOUN': {'VERB': 0.5, 'END': 0.5},\n",
       " 'VERB': {'NOUN': 1.0},\n",
       " 'AUX': {'NOUN': 1.0}}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"will Will google facebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will', 'Will', 'google', 'facebook']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = s1.split()\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_tags(sentence, states, emission_probs, transition_probs):\n",
    "    num_states = len(states)\n",
    "    num_words = len(sentence)\n",
    "    viterbi = np.zeros((num_words, num_states))\n",
    "    backpointer = np.zeros((num_words, num_states), dtype=int)\n",
    "\n",
    "    initial_probs = transition_probs.get('START', {state: 0 for state in states})\n",
    "    for s, state in enumerate(states):\n",
    "        viterbi[0, s] = initial_probs.get(state, 0) * emission_probs[state].get(sentence[0], 0)\n",
    "\n",
    "    for t in range(1, num_words):\n",
    "        for s, state in enumerate(states):\n",
    "            max_prob, max_state = max(\n",
    "                (viterbi[t-1, prev_s] * transition_probs.get(states[prev_s], {}).get(state, 0) * emission_probs[state].get(sentence[t], 0), prev_s)\n",
    "                for prev_s in range(num_states)\n",
    "            )\n",
    "            viterbi[t, s] = max_prob\n",
    "            backpointer[t, s] = max_state\n",
    "\n",
    "    # Backtrack to find the best path\n",
    "    best_path = np.zeros(num_words, dtype=int)\n",
    "    best_path[-1] = np.argmax(viterbi[-1])\n",
    "\n",
    "    for t in range(num_words - 2, -1, -1):\n",
    "        best_path[t] = backpointer[t + 1, best_path[t + 1]]\n",
    "\n",
    "    best_tags = [states[state] for state in best_path]\n",
    "\n",
    "    return print(best_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VERB', 'VERB', 'VERB', 'AUX']\n"
     ]
    }
   ],
   "source": [
    "predicted_tags = predict_tags(new, POS_tags, em_probabilities, trans_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
